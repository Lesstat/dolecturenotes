# -*- org-confirm-babel-evaluate: nil; -*-
#+Title: Discrete Optimization
#+filetags: DO studium


* Network Flow
- Given a directed graph G(V,E)
- with capacity cap: E -> N_0
#+BEGIN_SRC dot :file images/graph1.jpg :exports results
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label="10"] 
          s -> b [label="5"]
          a -> b [label="15"]
          a -> t [label="5"]
          b -> t [label="10"] 
  }
#+END_SRC

#+RESULTS:
[[file:images/graph1.jpg]]

- Goal: compute a maximum flow f from $s \in V$ to $t \in V$

** Definition Flow
A flow f is function f: E -> $N_0$ such that
- $\forall e \in E: f(e) \leq c(e)$ with c as capacity
  - This is called "capacity conservation"
- $\forall v \in V \backslash \{s,t\}$:
  - $$\sum_{e=(.,v)}f(e) =  \sum_{e=(v,.)}f(e)$$
  - this is called "flow conservation"
In other words the goal is to find a flow f  which maximizes $$\sum_{e=(s,.)}f(e)$$

** Examples

#+BEGIN_SRC dot :file images/graph2.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label=<<table border="0"><tr><td><font color="red">7</font> 10</td></tr></table>>]
          s -> b [label=<<table border="0"><tr><td><font color="red">3</font> 5</td></tr></table>>]
          a -> b [label=<<table border="0"><tr><td><font color="red">8</font> 15</td></tr></table>>]
          a -> t [label=<<table border="0"><tr><td><font color="red">6</font> 5</td></tr></table>>]
          b -> t [label=<<table border="0"><tr><td><font color="red">12</font> 10</td></tr></table>>]
  }
#+END_SRC

#+CAPTION: Network with flow. Flow in red, capacity in black
#+RESULTS:
[[file:images/graph2.jpg]]
- capacity constrained violated at:
  - (a,t) (b,t)
- flow constrained violated at:
  - node a & b (Input not equal output 7 != 14 & 11 != 12)

#+BEGIN_SRC dot :file images/graph3.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label=<<table border="0"><tr><td><font color="red">7</font> 10</td></tr></table>>]
          s -> b [label=<<table border="0"><tr><td><font color="red">2</font> 5</td></tr></table>>]
          a -> b [label=<<table border="0"><tr><td><font color="red">2</font> 15</td></tr></table>>]
          a -> t [label=<<table border="0"><tr><td><font color="red">5</font> 5</td></tr></table>>]
          b -> t [label=<<table border="0"><tr><td><font color="red">4</font> 10</td></tr></table>>]
  }
#+END_SRC

#+RESULTS:
[[file:images/graph3.jpg]]
- valid flow value of 9 (amount that leaves s / goes into t)
- flow on (s,b) and (b,t) could be increased to achieve an even better
  flow value
- _easy_: check whether a flow value is valid or feasible
- _much harder_: check whether flow is maximum value
- But even if we could check for optimality how to compute the maximum
  flow is unclear.

** "Natural" Algorithm 
- find a path from s to t where we can send as much flow as possible.
#+BEGIN_SRC dot :file images/graph4.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label="10", color="red"] 
          s -> b [label="5"]
          a -> b [label="15", color="red"]
          a -> t [label="5"]
          b -> t [label="10", color="red"] 
  }
#+END_SRC

#+CAPTION: Network with path that can send maximum flow in red.
#+RESULTS:
[[file:images/graph4.jpg]]
- send 10 units of flow on red path
#+BEGIN_SRC dot :file images/graph5.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> b [label="5"]
          a -> b [label="5"]
          a -> t [label="5"]
  }
#+END_SRC
#+CAPTION: Remaining capacities in the network
#+RESULTS:
[[file:images/graph5.jpg]]
- It seams that no more flow can be sent from s to t since the
  remaining capacities do not allow for additional capacity/flow to be
  sent 
- But this flow of value 10 is not optimal, as we have already seen a
  flow of value 12. This can be fixed by the following idea:
  - If flow is sent on an edge, it is possible to sent flow into the
    opposite direction by reducing the sent flow.
- So algorithm works now as follows:
#+BEGIN_SRC dot :file images/graph6.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label=<<table border="0"><tr><td><font color="red">10</font> 10</td></tr></table>>]
          s -> b [label=<<table border="0"><tr><td>5</td></tr></table>>]
          a -> b [label=<<table border="0"><tr><td><font color="red">10</font> 15</td></tr></table>>]
          a -> t [label=<<table border="0"><tr><td>5</td></tr></table>>]
          b -> t [label=<<table border="0"><tr><td><font color="red">10</font> 10</td></tr></table>>]
  }
#+END_SRC
#+CAPTION: Current Flow value
#+RESULTS:
[[file:images/graph6.jpg]]
- now construct residual network:
#+BEGIN_SRC dot :file images/graph7.jpg
  digraph g{
          rankdir=LR;
          {rank=source s}
          {rank=sink t}
          {rank = same; a; b}
          a -> s [label=<<table border="0"><tr><td><font color="green">10</font></td></tr></table>>, color="green"]
          s -> b [label=<<table border="0"><tr><td><font color="red">5</font> 5</td></tr></table>>, color="red"]
          a -> b [label=<<table border="0"><tr><td>5</td></tr></table>>]
          b -> a [label=<<table border="0"><tr><td><font color="green">10</font> <font color="red">5</font></td></tr></table>>, color="green", fontcolor="green" ]
          a -> t [label=<<table border="0"><tr><td><font color="red">5</font> 5</td></tr></table>>, color="red"]
          t -> b [label="10",color="green", fontcolor="green"]
  }
#+END_SRC
#+CAPTION: Residual Network: Back-edges in green. Next used path in red.
#+RESULTS:
[[file:images/graph7.jpg]]
#+BEGIN_SRC dot :file images/graph8.jpg
  digraph g{
          rankdir=LR;
          {rank = same; a; b}
          s -> a [label=<<table border="0"><tr><td><font color="red">10</font> 10</td></tr></table>>]
          s -> b [label=<<table border="0"><tr><td><font color="red">5</font> 5</td></tr></table>>]
          a -> b [label=<<table border="0"><tr><td><font color="red">5</font> 15</td></tr></table>>]
          a -> t [label=<<table border="0"><tr><td><font color="red">5</font> 5</td></tr></table>>]
          b -> t [label=<<table border="0"><tr><td><font color="red">10</font> 10</td></tr></table>>]
  }
#+END_SRC

#+CAPTION: Maximum flow value of 15
#+RESULTS:
[[file:images/graph8.jpg]]

** Max Flow
*** Ford Fulkerson-Algorithm
- Def:
  - For a given network $G(V,E,cap)$
  - and a valid flow $f: E -> \mathbb{N}$
  - the _residual network_ $G_f(V, E_f, cap_f)$ is defined as follows:
    - $\forall e \in E$ with $cap(e) > f(e)$ we have $e \in E_f$ with
      $cap_f(e) = cap(e) - f(e)$
    - "if there is capacity left on edge, this edge is also in $E_f$
      with remaining capacity"
    - $ \forall e=(v,w) \in E$ with $f(e) > 0$ we have  $e_R = (w,v) \in
      E_f$ with $cap_f(e_R) = f(e)$ [fn:1]
    - "if currently x unit of flow are sent form v to w, we can
      virtually send x units form w to v by not sending anything from
      v to w"
- Algorithm:
  1. start with zero flow $f(e) = 0: \forall e \in E$
  2. Construct the residual network $G_f$
  3. Find a *augmenting* path from s to t in $G_f$, send as much flow across that
     path (as determined by its bottleneck edge of minimum capacity)
     and add this flow to f.
  4. If no such path can be found return f. Otherwise goto 2
- For _integral_ capacities this algorithm eventually terminates,
  since in each round the flow increases by at least 1, but is also
  upper bounded by $$\sum_{e=(s,.)}cap(e)$$ as well as
  $$\sum_{e=(.,t)}$$[fn:2]

#+CAPTION: Example for bad choice of augmenting path. Left side shows residual network G_f, right side only the current flow
#+ATTR_LaTeX: scale=0.50
#+Label: fig:ff_bad_example
[[file:images/Fig_02_edited.jpg]]
- Why choosing the right path is important is illustrated by figure
  ref:fig:ff_bad_example 
- this continues until there is no path from s to t in G_f in this
  example the unclever choice of the augmenting path leads to a 1998
  augmentations.
- more generally to a algorithm with running time $O(val(f_{opt}) * (m+n))$
- $f_{opt}$ : value of maximum flow
- $(m+n)$: Depth-first-search or Breadth-first-search[fn:3]
- not polynomial because $val(f_{opt}$) is not polynomial (depends on
  encoding of the problem)

**** Is the algorithm correct?
- simple:
#+CAPTION: Prove for optimality by making a cut
#+LABEL: fig:ff_optimality
[[file:images/Fig_03_edited.jpg]]
- Figure ref:fig:ff_optimality shows a cut that can be used to proof
  optimality of the flow. [fn:4]
- partition of the nodeset into 2 groups which separate s and t.
- to sent flow from s to t the flow has to cross the boundary between
  the partition
- all the crossing edges have maximum flow
- Their summed capacity is 120 which is an upper bound for the flow
- Any (such?) cut implies a upper bound for the flow
- the value of the max flow = MinCut
- Definition:
  - Let G(V,E) be a directed graph, $\emptyset \neq A \not\subseteq V$. 
  - the _directed cut_ induced by A is $dcut(A):= \{e=(v,w) | v\in A,
    w \notin A\}$ that is, the set of edges having their source in A
    and their target in V\A
- Lemma:
  - For a directed graph G(V,E) with capacities $cap: E -> \mathbb{N}$, let $A
    \subseteq V$  with $s \in A, t \in V \backslash A$
  - Then $$\sum_{e \in dcut(A)} cap(e)$$ is an upper bound on the
    maximum flow value from s to t.
- Pf:
  - Consider any (maximum) flow form s to t. Each unit of flow has to
    cross the boundary form A to V\A at least once.
  - But in total at most $$\sum_{e \in dcut(A)} cap(e)$$ units of flow
    can cross the boundary.
- High-level ideal of correctness Proof from Ford-Fulkerson: look at
  last residual network and use this to derive a directed cut of value
  = current flow
- In detail:
  - Let us take as A the set of nodes reachable from s in the last
    residual network before termination
  - Note. $t \notin A$ since we terminated.
- Lemma:
  - Let f be the flow produced by FF for a network G(V,E,cap) and let
    e=(v,w) be such that $|\{v,w\} \cap A| = 1$ (e is out of or into A)
  - Then the following holds
    - if $v \in A$ (e is outgoing of A) then f(e) = cap(e)
    - if $v \notin A$ (e is incoming into A) the f(e) = 0

#+CAPTION: Cut A which includes all nodes reachable from s
#+LABEL: fig:ff_cut
    [[file:images/Fig_04_edited.jpg]] 
- Proof:
  - $v \in A, w \notin A, e=(v,w)$ if f(e) < cap(e) then the edge
    (v,w) is still present in G_f
  - => if $v \in A$ the also $w \in A$ -> contradiction
  - $v \notin A, w \in A, e=(v,w)$ if f(e) > 0 then the edge $(w,v) \in G_f$
  - =>  $v \in A$ -> contradiction
  - qed
- This immediately implies correctness of FF as the flow computed by
  FF has value $$\sum_{e=(v, w), v \in A, w \notin A} f(e) =
  \sum_{e=(v, w), v \in A, w \notin A} cap(e)$$
- second sum is the upper bound for flow by the set A [fn:5]

**** Capacity scaling
- Idea
  - First look only for augmenting paths with somehow "large"
    bottleneck capacity, only if no such paths exist, look for smaller
    bottleneck capacities
  - start with zero flow f=0
  - D = next smaller power of 2 of the max. capacity = $2^{\lfloor
    log_2(cap_{max}) \rfloor}$
  - ~while~ $D \geq 1$ ~do~
    - $G_f^D$ <- residual network of G wrf f, restricted to edges of
      capacity $\geq D$, 
    - ~while~ $\exists$  an augmenting path in $G_f^D$ ~do~
      - augment f
      - recompute $G_f^D$
    - ~od~
    - D = D/ 2
  - ~od~
  - ~return~ f
- Obviously, algorithm is correct since at some point D = 1 and this
  is plain FF.
- Lemma:
  - Let $f_D$ be the flow after completing augmentations with bound
    D. Then the value of max flow is bounded by the $val(f_D) + m * D$
- Proof: -> *wrong Proof -> Compare to scribe notes*
  - Consider the residual network $G_f^D$ after the last augmentation
    for value D. and let A be the set of nodes from s in $G_f^D$
  - Then consider the original network G, the flow $f_D$ and the set
    A. All edges leaving A with $cap \geq D$ are fully saturated and
    yield a flow of value $val(f_D)$. There are at mots m edges of
    capacity < D leaving A.
  - => one can send at most m*D across the boundary from A to V\A additionally.
  - qed
- When Algorithm reaches the end of the outer loop, the Lemma tells us that the
flow can grow by at most m*d in _all_ future iterations of the outer
loop. 
- But in the next round (with value D/2) in each successful iteration of the
inner loop, the flow grow by at least D/2.
- => The iteration of the outer loop ca have at most $\frac{ m*D}{D/2} = 2m$
- Theorem: FF with capacity scaling has running time of
  $O((log(cap_{max}))*m*(m+n)) = O(m^2 log(cap_{max}))$ 
  - $log(cap_{max})$: # iterations of outer loop
  - m : # iterations of inner loop
  - (m+n): cost for single augmentation
    - Normal FF $O(( m+n )*cap_{max}) = O(m^2 cap_{max})$ -> much slower

**** Successive Shortest Path (Edmonds-Karp-Algorithm)
- Consider the variant of FF where we always choose a shortest (wrt #
  edges) path from s to t, that is, the standard variant with BFS to
  determine the augmenting path.
- Lemma 6:
  - during the course of the algorithm, the length of the augmenting
    paths never decreases.
- Lemma 7:
  - After at most O(m) augmentations, the length of the augmenting
    path increases by at least 1.
- Theorem:
  - FF with shortest path augmenations terminates after $O(m^2 *n)$ steps.
- Proof:
  - The first augmenting path has length at least 1. After O(m)
    augmentations it has length at least 2, ... the augmenting path
    can have length at most n 
  - => O(n*m) augmentations, each of which cost O(m+n)
- Proof(Lemma 6):
  - Let l(v) be the hop distance of v from s in the residual network,
    G_l the subgraph of the residual network which only contains edges
    (u,v) with l(v) = l(u)+1
  - When augmenting augmenting along path \pi, two things can happen
    along this path:
    - (a) edges in the residual network disappear (as their capacity
      is fully used up)[fn:6] 
    - (b) back/ virtual edges are created that have not been present
      before.[fn:7]
  - None of these operations can decrease the level of a node.
  - => Distance to any node in particular t never decreases
- Proof(Lemma 7)
  - Le E_k be the set of edges at the beginning of a phase[fn:8] when the
    distance between s and t is k.
  - As soon as the shortest path  from s to t uses an edge not in E_k,
    it has length > k. Since in every augmentation step at least on
    edge (the bottleneck edges) is eliminated[fn:9] from E_k, after at
    most m steps the length of the shortest path from s to t is increased.

** MinCostFlow (Better MinCostMaxFlow Problem)
[[file:images/Fig_08_edited.jpg]]
- Every node can either be a source or a sink
- Example
[[file:images/Fig_09_edited.jpg]]
  - is a valid flow as:
    - on every edge we do not exceed the capacity constraint
      - $\forall e \in G: f(e) \leq cap(e)$
    - Flow conservation are satisfied:
      - $$\forall v \in G: \sum_{e=(v,.)}f(e) = \sum_{e=(.,v)}f(e) +b(v)$$
      - b(v) demand/supply of node v ( flow entering or leaving at node v )
  - This flow has cost of:
    - $$\sum_{e \in E}f(e)*cost(e)$$
    - $=f(ab)*cost(ab)+ f(ac)*cost(ac)+f(bc)*cost(bc)+f(bd)*cost(bd)+f(cd)*cost(cd)$
    - $= 2*2 + 2*2 + 1 *10 + 2*3 + 0*1 = 24$
*** Formal Problem Definition
Given a directed network G(V,E) witch capacities $cap: E \rightarrow
\mathbb{N}$ and $cost:E \rightarrow \mathbb{Z}$ and a function $b:V
\rightarrow \mathbb{Z}$ which determines whether a node v has a
surplus/ supply of flow (b(v)>0) or a demand of flow (b(v)<0) with
- $$\sum_{v:b(v)>0}b(v) = - \sum_{v:b(v)<0} b(v)$$
- we want to find a flow $f: E \rightarrow \mathbb{N}$ such that
  - $\forall e \in E:  0 \leq f(e) \leq cap(e)$ "capacity constraint"
  - $$\forall v \in V: b(v) + \sum_{e=(w,v)}f(e) = \sum_{e=(v,w)}f(e)$$
    "flow conservation constraint"
  - and minimizing $$\sum_{e\in E}f(e)*cost(e)$$
*** High level Algorithm idea:
1. Find some feasible flow (-> exercise) (not caring about cost)
2. Optimize cost of the flow
*** ad 2. Idea
- "Reroute" flow as long this reduces the costs without violating constraints
- One possibility to reroute flow without violating flow conservation
  constraints is sending flow in a cycle.
  - the flow for Nodes not in the cycle doesn't change
  - the flow for Nodes on the cylce they gain as much as they loose
- If sending flow along a cycle decreases total cost we should do so.
- As in maxflow we consider the residual network $G_f(V,E_f)$ for a
  given network G(V,E) and flow f.
- For each edge $e \in E$ we construct up to two edges in $E_f$ as follows:
  - if f(e) < cap(e) then there is an edge $e' = (v,w) \in E_f$ with
    - cost $cost(e')= cost(e)$ and
    - $cap(e') = cap(e) - f(e)$
  - if f(e)>0 then there is a virtual edge $e'' =(w,v)\in E_f$ with
    - $cap(e'') = f(e)$ and
    - $cost(e'') = - cost(e)$
- Example
  - *Exersice* use cycle acdba instead and cont cycle cancelling until termination
[[file:images/Fig_10_edited.jpg]]

  - There is a negative cycle acba of cost -10.
  - For every unit of flow sent across that cycle the total cost
    decreases by 10
  - We can send up to 1 unit of flow around (due to bottleneck edge ac)
  - resulting flow and residual network, which contains no negative cycle
[[file:images/Fig_11_edited.jpg]]
  - Alg. finishes  [fn:10]
- Algorithm is also called "cycle cancelling" algorithm as once flow
  is sent across negative cycle, it will not be present in the next
  residual network anymore.
- Clearly, algorithm terminates as cost decreases by at least 1 per
  cycle cancelling.
*** Proof
- Lemma
  - Let f be a valid flow in G, $G_f$ the respective residual network.
  - Then, f is a minCost flow if and only if $G_f$ does not contain a
    negative cylce.
- Proof:
  - The ''=>'' is trivial
  - ''<='' assume $G_f$ does not contain a negative cycle but f is not a
    minCostFlow. We will derive a contradiction hence proofing the Lemma.
  - Let $f^*$ be a minCost flow and consider the flow-difference f' =
    $f^* - f$
  - we claim f' has to contain a negative cycle
  - Clearly, cost(f')< 0 since $cost(f^*)< cost(f)$
  - Furthermore f' must satisfy flow conservation (inflow = outflow),
    that is at any v, the outflow of f' must equal the inflow of f'.
 [[file:images/Fig_12_edited.jpg]]
  - Hence f' can be decomposed into a set of cycles
 [[file:images/Fig_13_edited.jpg]]
  - Since the total cost of the cycles is negative, one of the cycles
    must have negative cost.
  - This cycle is also a negative cost cycle in $G_f$
  - this is a contradiction
*** Problem to solve
- Given a directed weighted graph G(V,E,c) with edge costs
  $c:E\rightarrow \mathbb{Z}$ find a negative cycle.
- Idea to find negative cycles:
  - Create n+1 copies of the V
 [[file:images/Fig_14_edited.jpg]]
  - Connect node v in layer (i) with node w in layer (i+1) iff [fn:11]
    $(v,w) \in E$ (same costs)
  - Then $\forall v_i$ compute shortest path distances from $v_i^{(1)}$
    to all other nodes in this layered graph.
  - if there exists a node $v_i^{(j)}, j > 1$ with distance <0, we have
    found a negative cycle starting in $v_i$
- Running Time:
  - Computing shortest path distances from $v_1^{(1)}$ costs O(n*m)
  - => in total we can find a negative cycle $O(n^2m)$
- Finding the "most negative" cycle (which seems desirable to decrease
  cost as much as possible) is NP-hard unfortunately, reduction via
  Hamilton Cycle Problem [fn:12].
*** Possible Running Time Improvements
- Finding the cycle C minimizing $\frac{cost(c)}{|C|}$, that is the
  min mean cost cycle (in fact using the same algorithm).
- If always the min mean cost cycle is cancelled, one can guarantee
  polynomial running time of the cycle cancelling algorithm. (no
  details here/ not part of the exam)
*** Application of min-Cost-Flow
- Consider a set of jobs J and a set of workers W.
- Each worker $w \in W$ has the qualification to perform jobs $J_w
  \subseteq J$
- Question
  - Is there an assignment of jobs to workers such that 
  - every job is done
  - no worker performs more than one job
  - no worker performs a job he is not qualified for
 [[file:images/Fig_15_edited.jpg]]
- Extension with costs
  - worker w performing job j incurs cost of c(w,j)
  - Goal: Find assignment of jobs to works minimizing total cost.
  - add costs c(w,j) to edge e=(w,j). Use min-Cost-Flow
- Heiratssatz (not relevant for Exam)
 [[file:images/Fig_16_edited.jpg]]
  - Q: Is there an assignment of men to women such that everyone is
    married (without fighting)
  - Hall's Theorem:
    - Assignment possible if and only if $\forall S \subseteq M: |
      N(S)| \geq |S|$ [fn:13]
**** Airplane hopping
- Consider a small airline with only one plane serving n-1 cities in a
  round trip every day.
- The airline makes money by picking up passengers at $v_i$ and dropping
  them off at $v_j$
- Unfortunately the plane is not big enough to transport all passengers.
- Assume there are
  - $t_{ij}$ passengers who want to go from $v_i$ to $v_j$ willing to pay a
    fare of
  - $f_{ij}$ for that
  - the plane has p seats.
- which passengers should be picked up to maximize the revenue?
 [[file:images/Fig_17_edited.jpg]]
- e.g:
  - 2 from $v_1$ -> $v_3$ => 6 €
  - +    1 for $v_2$ -> $v_4$ => 4 €
  - sums to 10 €
  - OR
  - 3 from $v_2$ -> $v_4$ => 12€
- Modelling as MinCostFlow Problem
- Nodes i,j represents passengers who want to go from i to j. [fn:14]
*** Alternative Approach to MinCostFlow (via successive shortest path)
- Recall the Cycle Cancelling Flow Approach
  1. Find some feasible flwo (e.g. FF)
  2. "Kill" negatvice cost cycles from residual networks as long as possible
- Always maintain feasibility, steer the solution towards optimality
- Now
  - Try to always keep a cost optimal solution (for part of the
    problem), but iteratively cover more and more of the problem until
    feasibility is reached [fn:15].
  - We can compute the minCostFlow also via a simple variation of
    Ford-Fulkersion (!)
  - where in each round we choose as augmenting path the cheapest path
    in the residual network connecting a supply node with a demand node.
[[file:images/Fig_18_edited.jpg]]
  - Such a path can be found via a shortest path computation on a
    slightly enhanced residual network (see picture above)
  - Bellman-Ford can be used for that
  - Eventually the flow will become feasible[fn:16] (and cost-optimal).
- Proof-Idea: (for optimality)
  - Show that all residual networks occuring throughout the course of
    the algorithm have no negative cycles.
  - The first residual network obviously has no neg. cycles
  - Now consider the residual network after first augmentation and
    assume it contains a negative cycle C.
  - Decompose C into $C_1$ and $C_2$ where $C_1$ starts and ends with
    negative cost edges and $C_2$ only contains edges with
    non-negative costs
[[file:images/Fig_19_edited.jpg]]
  - Red edges have negative cost
  - $cost(C_1)$ < 0 and $|cost(C_1)|$ > $cost(C_2)$
  - For simplification assume that C_1 consists solely of negative
    cost edges which also form a continuous reverse subpath of the first
    augmenting path.
  - Note that $C_2$ yields a path from $v_2$ to $v_1$ of $cost(C_2)$
    < $|cost(C_1)|$
  - contradiction to choice of augmenting paths as always cost-minimal
    path
- Now we know that the 1st (trivial) and 2nd residual networks contain
  no negative cycles we can extend this to every following residual
  network by applying the Johnson Shifting Theorem (1975)
- Theorem
  - Let G(V,E,c) be a directed weighted graph with possibly negative
    edge costs but without negative cycles. Then there exists a
    potential function $\phi: V -> \mathbb{Z}$ such that the shifted
    edge costs:
    - $c'(v,w) = c(v,w) + \phi(v) - \phi(w)$
    - are all non-negative and shortest paths in G(V,E,c) remain
      shortest in G(V,E,c') and vice versa.
- Proof: 
  - w.l.o.g assume $\exists s \in V$ from which all other node
    can be reached. 
  - Since G(V,E,c) is free of negative cycles, shortest
      patht distances are well-defined and can be computed using
      Bellman-Ford in O(mn) time.
  - Then define $\phi(v) = d_s(v)$.
  - First observe that due to d_s() being shortest path distances we
    have
  - $d_s(w) \leq d_s(v) + c(v,w)$ for every edge (v,w)
  - => $c'(v,w) = c(v,w) + \phi(v) - \phi(w)$
  - $= c(v,w) + d_s(v) - d_s(w)$
  - $\geq 0$
  - $c(v,w) \geq d_s(w) - d_s(v)$
  - => all new edge costs are non-negative
  - now consider some path $\pi = sv_0v_1...t$ in G(V,E,c'). The cost
    of $\pi$ is:
    - $$\sum_{(v,w) \in \pi}c'(v,w) = \sum_{(v,w) \in \pi} c(v,w) +
      \phi(v) - \phi(w)$$
    - $$= (\sum_{(v,w) \in \pi} c(v,w)) + \phi(s) - \phi(t)$$
    - So the cost of a path $\pi$ from s to t in G(V,E,c') is the cost
      in G(V,E,c) plus the potential of the source minus the potential
      of the target.
    - Since the latter two are invariant for different st-paths,
      shortest paths in G(V,E,c) are shortest paths in G(V,E,c') and
      vice versa.
    - qed.
- So we can apply this Johnson Theorem to 2nd residual network and
  argue again as before.

* Linear Programming
- Example in $\mathbb{R}^2$
  - LP is equivalent to "looking for the 'best' point satisfying some constraint"
[[file:images/Fig_20_edited.jpg]] 
- Example: Cheap and ''healthy'' diet
  - Healthy life means
    - at least 11 units of carbohydrates
    - at least 7 units of protein
    - at least 5 units of fat
  - These can be acquired by eating
    - meat, one unit of which has
      - 1 unit carbohydrates
      - 3 unit proteins
      - 5 units of fat
      - and costs 7 EUR
    - tofu,
      - 2 unit carbohydrates
      - 2 unit protein
      - 0 units of fat
      - and costs 3 EUR
    - bread (4,1,0) 2 EUR
    - cheese (1,4,2) 4 EUR
  - Goal: Determine how many units of tofu, bread, cheese or meat to
    buy every day such that we live a healthy life and minimize the cost.
  - We formalize this by introducing variables for the sought after
    quantities:
    - $X_m$: units of meat to buy every day
    - $X_t$: units of tofu to buy every day
    - $X_b$: units of bread to buy every day
    - $X_c$: units of cheese to buy every day
    - objective function
      - $\min 7X_m + 3X_t + 2X_b + 4 X_c$
    - constraints 
      - $1X_m + 2X_t + 4X_b + 1X_c \geq 11$ (carbohydrate constraint)
      - $3X_m + 2X_t + 1X_b + 4X_c \geq 7$ (protein constraint)
      - $5X_m + 0X_t + 0X_b + 2X_c \geq 5$ (fat constraint)
      - $7X_m + 3X_t + 2X_b + 4 X_c \geq 0$
  - In general, a linear program is given as (in Standard form):
    - $\max C_1X_1 + C_2X_2 + ... + C_nX_n$ [fn:17]
    - $a_{ 11 }X_1 + a_{ 12 }X_2 +... + a_{ 1n }X_n \leq b_1$
    - $a_{ 21 }X_1 + a_{ 22 }X_2 +... + a_{ 2n }X_n \leq b_2$
    - $\vdots$
    - $a_{ m1 }X_1 + a_{ m2 }X_2 +... + a_{ mn }X_n \leq b_m$
  - This is a maximization problem in n variables X_1, X_2, ... X_n n
    coefficients C in the objective function and m constraints given
    by the coefficient matrix $A \in \mathbb{R}^{m \times n}$ and
    right hand sides $b \in \mathbb{R}^m$ in short we write
    - $\max c^Tx$
    - $Ax \leq b$
  - The j-th constraint here is :
    - $$\sum_{i = 1}^n a_{ji}x_i \leq b_j$$
  - _Easy_: Transform any linear program into standard Form[fn:18]
** Geometric interpretation of linear programs 
- A linear program can be thought of as a polyhedron in n-dimensional
space. 
- A polyhedron is an intersection of halfspaces
  - in $\mathbb{R}^2$: halfspace ^= halfplane
[[file:images/Fig_27_edited.jpg]] [fn:19]
  - $\max x +y$
  - $-x -y \leq -3$ (orange)
  - $ 0.5x +y \leq 7$ (green)
  - $ 0.5x -y \leq 0$ (red)
  - $ -0.5x +y \leq 3$ (black)
  - Feasible Region in red
[[file:images/Fig_28_edited.jpg]] [fn:20]
- Remarks
  - The feasible region defined by an LP can be empty
[[file:images/Fig_29_edited.jpg]]
  - The feasible region might also be unbounded (as well as the
    optimal solution)
[[file:images/Fig_30_edited.jpg]]
- Lemma
  - If the feasible region is a bounded, non-empty polyhedron, the
    maximum objective function value is (also) obtained at a corner
    (vertex) of the polyhedron.
- Proof
  - Follows from convexity & exercises
- corner of a polyhedron
  - Intersection of n linearly independent constraints ( their
    respective bounding hyperplane)
  - in $\mathbb{R}^2$
[[file:images/Fig_31_edited.jpg]]
** The (primal) Simplex Algorithm
- Assume that C = (0,0,...,-1) that is, we are always looking for the
  ''lowest'' feasible point.
[[file:images/Fig_32_edited.jpg]]
- Except for dimensions 1,2,3 it is very inefficient to just compute
  the feasible reason explicitly and inspect all corners.
- In general, a polyhedron in dimension n defined by m halfspaces can
  have complexity $m^{ \lfloor \frac{n}{n}\rfloor}$
- Why does simplex algorithm terminate? (assuming a corner is defined
  by exactly n constraints)
- There are only finitely many  < $ m \choose n$ corners and we
  never visit one twice.
- ... unknown content
\begin{equation*}
  \begin{pmatrix}
   a_{11} \dots a_{1d}  \\
   \vdots  \ddots \vdots \\
   a_{d1} \dots a_{dd} \\
  \end{pmatrix}
  \begin{pmatrix}
    x_1 \\
    \vdots \\
    x_d
  \end{pmatrix}
  +
  \begin{pmatrix}
    s_1 \\
    \vdots \\
    s_d
  \end{pmatrix}
  =
  \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_d
  \end{pmatrix}
\end{equation*}
with $s_i \geq 0$
- In short we write $A_B x + S =b_B$
- The coordinates of the current certex v are determined
  $x=A_bb_B-A_BS$
- Move away from vertex v on a ray bounding the feasible region
  corresponds to increasing one of the slack variables to a non-zero
  value while keeping the others zero.
- We want to see which slack variable is worth increasing <=> which
  constraint is worth moving away from to improe the objective
  function value.
\begin{align*}
\underbrace{ c^Tx }_{\substack{ \text{objective value of point x } \\ \text{ moving along one of the rays  }}}  &= c^T (A_B^{-1}b_B - A_B^{-1}s)\\
& = c^T\underbrace{x'}_{\substack{\text{coordinates of vertex} \\ \text{I was sitting on (hence x')}}} - 
\underbrace{\alpha_1s_1 - \alpha_2s_2 - ... \alpha_d s_d }_{-c^TA_Bs?}
\end{align*}
- => Expressed dependence on s. The objective function value is a
 constant (the obj. func. value at vertx v) and a weighted sum of the
 $s_i$ with weights $(\alpha_1,... \alpha_d)$.
- For each i with $\alpha_i < 0$, moving away from the respective
  constraint (=> increasing $s_i$) improves the objective function value.
- If no such i exists no neighboring corner can be better => optimium reached.
- Now assume we have a constraint $a_i^Tv \leq b_i$ which we want to
  move away from (this constraint leaves the basis). We need to finde
  the new constraint entering the basis, that is, the constriant that
  is violated when moving along the ray.
- $x' = A_B^{-1}b_B$ is the current/old vertex we want to move a way from.
- $$x'' = A^{-1}b_B - \underbrace{ (A_B^{-1})_{ .i } }_{ \text{ith column of $A_B^{-1}$}}$$
- We can express the improving ray as
  $\vec{r}=x'+\lambda(x''-x')=x'-\lambda(A_B^{-1})_{.i}$
- we are .. in the constraint violated when increasing $\lambda$ (=
  moving along the ray), so for every constriant l we can do the
  following: $a_l^T(x'-\lambda(A^{-1}_B)_{.i}) \leq b_l$
- and check when this inequality is fulfilled with equality
- <=> $a_l^Tx' - \lambda a^T (A^{-1}_B)_{.i} < b_l$
- if $a_l^T (A^{-1}_B)_{.i} \geq 0$ this constraint will never be
  violated, otherwise
- $$\lambda \leq \frac{b_l - a_l^Tx'}{-a_l^T  (A^{-1}_B)_{.i}}$$
- The constraint which yields the smallest upper bound on $\lambda$ is
  the new constraint entering the basis.
*** Pivoting Strategies
- Often there are several rays moving along improving the objective
  function value
- Natural Strategies:
  - Choose the steepest ray which improves the objective function
    value at the highest rate (cheap to figure out)
  - Choose the ray whose other vertex has the best objective function
    value (expensive as it requires all other neighbouring vertices on
    improving ray, but might make more progress)
- Theoretically no really good pivoting strategy is known, for many
  used strategies bad examples are known (e.g. Klee-Morty, Cube?)
[[file:images/Fig_25_edited.jpg]]
[[file:images/Fig_26_edited.jpg]]
- There is a randomized strategy with sub-exponential (not polynomial)
  running time.
- Pivoting rules are also important to deal with degenerate configurations
[[file:images/Fig_34_edited.jpg]] [fn:21]
- In higher dimensions it can happen, that one keeps sitting in the
  same corner only exchanging the constraints of basis -> no termination!
- _Remedy_ there is a special pivoting rule called Bland's rule
  (essentially kick out constraint with smallest index) guarantees
  that no basis appears twice and hence eventually we leave the
  corner. [fn:22]
** Duality
- Example:
  - Consider the old example LP
  - $\max x+y$
  - s.t 
    - $-x -y \leq -3$
    - $0.5x+y \leq 7$
    - $0.5x-y \leq 0$
    - $-0.5x+y \leq 3$ 
    - $x \leq 6$
  - can be solved by simplex algorithm
  - Assume we do not want to do that; can we easily find _upper
    bounds_ to the optimal objective function value?
  - Yes, consider the constraints :
    - $0.5 x +y \leq 7$ (II) => *1.5 = $0.75x +1.5y \leq 10.5$
    - $0.5x - y \leq 0$ (III)=> *0.5 $= 0.25x -0.5y \leq 0$
    - Adding those
      - $1x +1y \leq 10.5$
  - So the optimal objective function value can never exceed 10.5!
  - We could do even better by
    - $0.5x+y \leq 7$ => *1  $ = 0.5x+y \leq 7$ 
    - $x  \leq 6$ => *0.5 $ = 0.5x \leq 3$
    - Addition
      - $x+y \leq 10$
- More generally, assume we have a LP in standard form
  - $\max c_1x_1 + c_2x_2 + .. c_nx_n$
  - st. $a_{ 11 }x_1 + a_{ 12 }x_2 + .... a_{ 1n }x_n \leq b_1$
  - $\vdots$
  - $a_{ m1 }x_1 + a_{ m2 }x_2 + .... a_{ mn }x_n \leq b_m$
  - $\max c^Tx$
  - $Ax \leq b$
  - n: dimension
  - m: number of constraints
- The goal is to pick m multipliers (one for each constraint, all
  $\geq 0$; =0 means constraint is not picked)
- $\min y_1b_1 + y_2b_2 + ... + y_mb_m$
- s.t.
  - $a_{11}y_1 + a_{21}y_2 + ... + a_{m_1}y_m = c_1$
  - $a_{12}y_1 + a_{22}y_2 + ... + a_{m2}y_m = c_2$
  - $\vdots$
  - $a_{1n}y_1 + a_{2n}y_2 + ... + a_{mn}y_m = c_n$
  - m: variables
  - n+m: constraints (non-negativity constraints are also counted)
  - y: multipliers 
- In short:
  - $\min b^Ty$
  - $A^Ty = c$
  - $y_i \geq 0$
- So for any primal LP:
  - $\max c^Tx$
  - $Ax \leq b$
- There is a related dual LP [fn:23]
  - $\min b^Ty$
  - $A^Ty = c$
  - $y_i \geq 0$
- By construction every feasible solution to the dual LP yields an
  upper bound on the objective function value of the primal (also
  called _weak duality_).
- It turns out that the optimum solution to the dual LP yields an
  objective function value _exactly_ matching the optimal objective
  function value of the primal (also called _strong duality_).
** Economic Interpretation of the Dual
- Consider nutrition LP from beginning
- objective function
  - $\min 7X_m + 3X_t + 2X_b + 4 X_c$
- constraints 
  - $1X_m + 2X_t + 4X_b + 1X_c \geq 11$ (carbohydrate constraint)
  - $3X_m + 2X_t + 1X_b + 4X_c \geq 7$ (protein constraint)
  - $5X_m + 0X_t + 0X_b + 2X_c \geq 5$ (fat constraint)
  - $X_m,X_t,X_b,X_c \geq 0$
- Let us adopt the view of a producer of nutrition pills. His goal is
  to set prices y_c, y_p, y_f for each type of pill maximizing his revenue:
  - $11y_c + 7y_p + 5_yf$
  - for each customer
  - But there are some constraints, e.g buying one unit of meat  (7
    EUR) should not be cheaper than buying:
    - 1 Carbo-pill at $y_c$ EUR
    - 3 protein-pills at $y_p$ EUR
    - 5 fat-pills at $y_f$ EUR
    - $1y_c + 3y_p + 5y_f \leq 7$
  - equally for tofu, cheese and bread
    - $2y_c + 2y_p + 0y_f \leq 3$$$
    - $4y_c + 1y_p + 0y_f \leq 2$
    - $1y_c + 4y_p + 2y_f \leq 4$
    - $y_c, y_p, y_f \geq 0$
  - Exercise verify that this is indeed dual of diet-LP.
** The Dual Simplex Algorithm
[[file:images/Fig_35_edited.jpg]] Primal Simplex
[[file:images/Fig_36_edited.jpg]]
- V-Shape: corner defined by d lin. indep. constraints which have
  bounded optimal solution
[[file:images/Fig_37_edited.jpg]]
- Pivoting Step in the dual Simplex
[[file:images/Fig_38_edited.jpg]]
- v is optimal for $\{h_1,h_2\}$ but $v \not\in h_3$
- for $\{h_1,h_2,h_3\}$ the optimum vertex is 3.
- start with a corner which is optimal for a subset of constraints
- As long as there are constraints that are still violated consider
  them to jump to a "higher" V-Shape
- Optimum reached once current V-Shape does not violate any other constraints
*** Characterization of a V-Shape
- [[file:images/Fig_39_edited.jpg]] -> V-Shape
- [[file:images/Fig_40_edited.jpg]] -> V-Shape
- [[file:images/Fig_41_edited.jpg]] -> not a V-Shape
- A corner (defined by n constraints) is V-Shape iff the objective
  function vector lies in the cone of the normal vectors of the
  respective constraints, that is
- $\exists y \in \matnbb{R}^m, y \geq 0$ with $A^T y = c$
\begin{align*}
\text{j-th constraint:} a_{j1}x_1 + a_{j2} x_2 + ... + a_{jn}x_n \geq b_j \\
\text{has normal vector} (a_{j1}, a_{j2},..., a_{jn})^T
\end{align*}
- The Dual Simplex Algorithm jumps from one V-Shape to another V-Shape
  to another V-Shape,...
- Each V-Shape corresponds to a (special[fn:26]) solution to
  $A^Ty=c, y \geq 0$.
- => can interpret dual simplex algorithm as a sequence of feasible
  solutions to the dual linear program.
*** Dual Simplex Algorithm in more detail
- Consider (primal) LP in standard from an n of its m constraints
  forming a V-Shape.
- A V-Shape can be seen as subset B of n constraints such that their
  unique vertex $x_0$ is at the same time the optimum solution to the
  reduced LP consisting only of constraints in B.
- If $x_0$ is feasible for all constraints (also those not in B) we
  clearly have the optimum solution.
- If $x_0$ violates some constriant $h_i \not\in B$, that is,
  $a_{i.}x_o > b_i$, we can use this to derive a next V-Shape by
  determining the lowest feasible point to the constraint set
  $B\cup\{h_i\}$. Naively this can be done by considering the
  $n+1\choose n = n$ subsets of $B\cup\{h_i\}$ of cardinality n,
  computing their intersection point and checking feasibility.
[[file:images/Fig_42_edited.jpg]]
- $B_{old} = \{h_1,h_2\}, B_{new} = \{h_1,h_3\}$
- The new V-Shape is the represented by n-1 constraints of the old
  V-Shape and the previously violated constraints. Dual Simplex
  algorithm continues until current V-Shape does not violate any
  constraint.
- Since (assuming non-degeneracy) every V-Shape is "higher" than the
  previous one, termination is guaranteed.
- Existence of a V-Shape at a certain height implies that no lower
  feasible point exists.
- A V-Shape whose vertex does not violate any of the other constraints
  hence has to be optimal.
- In general note that the constraints $A^Ty =c, y \geq 0$ of the dual
  linear program can be intrepreted as an invariant of the dual
  simplex algorithm (always being in a V-Shape).
- $b^Ty$ expresses the "height" of the current solution/V-Shape since
  - $y = (A^T_b)^{-1}c$ and hence
  - $b^Ty = b^T(A^T_B)^-1c = (A_B^{-1}b)^Tc=x'^Tc$
  - where $A_B$ is the submatrix corresponding to the constraints that
    define the current V-Shape and x' the intersection point of these
    constraints.
*** Pivoting Rules & Degeneracies
- As in the primal there can be degeneracies which lead to the new
  V-Shape not being really higher than the old V-Shape. This can
  happen if the direction of the objective function (primal)
  coincides with one or more of the normal vectors of the
  constraints.
- This might lead to cycling/non-termination of the dual simplex
  algorithm.
- Can be fixed with special pivoting rules like Bland's rules in the
  primal.
** Properties of Primal & Dual LP
- Our derivation of the dual LP immediately implies that any feasible
  solution to the dual is an upper bound on the objective function
  value of the primal (and vice versa).
[[file:images/Fig_43_edited.jpg]]
- this is called _weak duality_
- Our intuition of fesaible corners and V-Shapes also immediately
  yield _strong duality_, that is the optimum primal objective function value
  _equals_ the optimum dual objective function value.
- Easy:
  - Given optimum solution to primal
  - => derive optimal solution to dual LP
  - & vice versa
** Interpretation of the dual Simplex Algorithm as Primal Simplex Algorithm on the dual LP
- Primal LP
\begin{align*}
\max &( c_1x_1 +c_2x_2 + ... + c_n x_n  )\\
\text{s.t.} &a_{11}x_1 + a_{12}x_2 +... a_{1n}x_n \leq b_1 \\
\vdots
 &a_{m1}x_1 + a_{m2}x_2 +... a_{mn}x_n \leq b_m \\
\end{align*}
- Dual LP
\begin{align*}
\min &( b_1x_1 +b_2x_2 + ... + b_m x_m  )\\
\teyt{s.t.} &a_{11}y_1 + a_{21}y_2 +... a_{n1}y_m = c_1 \\
\vdots
 &a_{1m}y_1 + a_{2m}y_2 +... a_{nm}y_m = c_m \\
&y_i \geq 0
\end{align*}
- typically m >> n
- We can bring the dial LP into Standard form by creation for each "="
  constriants on "$\geq$" and one "$\leq$"constraint and the
  multiplying wit -1 if necessary.
- before: DLP had n+m constraints
- after: DLP has 2n+m constraints in dimension m.
- If we run primal Simplex algorithm on this LP in standard form, a
  corner is always represented by m linearly independent constraints
  fulfilled with equality.
- Note that n of these tight constraints are always formed by
  constraints from the set of 2n constraints derived from the
  equalities in the dual LP.
- A corner is hence defined by at most n constraints from the group of 2n
  constraints and $ \geq m-n$ from the ($y_i \geq 0$) group
- => There are at most n of the $y_i \geq 0$ constraints not fulfilled
  with equality (that is $y_i > 0$).
- These $\leq n$ non-negative $y_i$ select and combine as a conic
  combination $\leq n$ constraints whose weighted normal vectors add
  up to the objective function.
- Or: the objective function vector $\vec{C}$ is always in cone of the
  constraints selected by non-zero $y_i$'s;
- The respective objective function value of the vertex hence yields
  an upper bound on the objective function value in the primal.
- A pivot step in the (primal) Simplex Algorithm on the dual LP can be
  interpreted as identifying a constraint in the primal space
  violating the current V-Shape and replacing one current constraint
  by violating constraint. [fn:27]
*** Another explanation
- A corner is defined by m lin. indep. constraints fulfilled with equality
- from the group of 2n constraints a basis of a corner get at most n
  of them (due to linear independence requirement)
- => a corner needs $\geq m-n$ tight constraints from the group of
  ($y_i \geq 0$) constraints
- => at least m-n of the $y_i$ have to be zero
- => at most n of the $y_i$ can be non-zero.
** Properties of  Primal and Dual LP
- Primal (P)
\begin{align*}
\max c^Tx \\
Ax \leq b
\end{align*}
- Dual (D)
\begin{align*}
\min b^Ty \\
A^Ty =c \\
y_i \geq 0
\end{align*}
- Weak duality: $\forall$ feasible x' for (P) and feasible y' for (D)
  $c^Tx'\leq b^Ty'$
- Strong duality: Let $x^*$ be an optimal solution to (P) and $y^*$ an
  optimal solution to (D) $c^Tx^*=b^Ty^*$
\begin{align*}
\max 5x + 7y \\
x +y \leq 7 \\
x- y \geq 5 \\
x,y \in \mathbb{N}
\end{align*}
[[file:images/Fig_44_edited.jpg]]
[[file:images/Fig_45_edited.jpg]]
** Infeasibilty/ Unboundedness of Primal and Dual LP
- Primal has optimum solution => Dual has optimum solution
- Primal has feasible solution with arbitrary objective function value
  => Dual is infeasible (there is no V-Shape)
- Primal is infeasible => Dual can be feasible
  [[file:images/Fig_46_edited.jpg]]
- Primal is infeasible => but also possible
  [[file:images/Fig_47_edited.jpg]]
** Complementary Slackness Condition
- Lemma:
  - let $x^*$ and $y^*$ be optimum solutions to primal and dual LP in
    standard form.
  - Then we have: if $y_i^* > 0$, $a_ix^*=b_i$, that is, if in the
    dual optimum, the dual variable corresponding to the i-th
    constraint (in primal) is non-zero, the i-th constraint in the
    primal must be fulfilled with equality for $x^*$.
- Proof:
  - if $y_i^* > 0$, this means that i-th constraint in the primal
    is one of the constraints forming the highest V-Shape $\hat{=}$ lowest
    feasible point. All the constraints forming the V-Shape/feasible
    point are fulfilled with equality
** Running Time Considerations
- Primal & Dual Simplex Algorithms typically fast in practice. But no
  proof showing that some pivot rule yields polynomial running time
  (in n & m). For fixed/constant dimension, Linear Programming can be
  solved in O(m) time. (with a constant ~ $2^n$).
- There are random pivot rules which guarantee subexponential (but not
  polynomial) running time. (e.g $O(2^{\sqrt{n}}*m)$)
- There are completely different approaches like interior point or
  Ellipsoid method which guarantee polynomial running time (in the
  size of the problem encoding).
** Idea of the Ellipsoid Method[fn:28] 
- Only tries to find a feasible point (not necessarily on the boundary
  but somewhere)
[[file:images/Fig_48_edited.jpg]] [fn:29]
- if that is possible in poly-time we can find opt. solution in
  poly-time by binary search
- How to determine feasible point?
file:images/Fig_49_edited.jpg
- Invariant: Always maintain ellipsoid containing feasible region
- two cases:
  1) center of current ellipsoid is feasible => we are done!
  2) center of current ellipsoid violates some constraint $a_ix \leq
     b_i$ => construct new ellipsoid whose volume is by at least a
     constant factor smaller than volume of old ellipsoid and still
     contains feasible region (if there is any)
file:images/Fig_50_edited.jpg
** LP-Based Approximation of NP-hard Problems
- Running Example: The Set Cover Problem
  - Given a universe U of n elements and a collection of subsets
    $\mathcal{S} =\{S_1,S_2,.. S_u\}, S_i \subseteq U$
  - and a cost function c: $\mathcal{S} \rightarrow \mathbb{R}^+$
  - the goal is to find a subcollection
    $\mathcal{S}'\subseteq\mathcal{S}$ such that
    - $$\bigcup_{S_i \in \mathcal{S}} S_i = U$$
    - and $$\sum_{S_i \in \mathcal{S}} c(S_I)$$ is minimal
- Example
  - $U = \{a,b,c,d,e,f,g\}$
  - $\mathcal{S} = \{ \{a,c,e,g\}, \{a,b,c\}, \{c,d,e\}, \{e,g\}, \{a,f\}\}$
  | i | c(S_i) |
  |---+--------|
  | 1 |      5 |
  | 2 |      7 |
  | 3 |      1 |
  | 4 |      9 |
  | 5 |      3 |
  - One feasible solution: $S_1,S_2,S_3,S_4,S_5$ with cost 25.
  - better solution: $S_1, S_2,S_3,S_5$ with cost 16
    - (apparently optimal)
- This problem is incredibly hard; it is not only NP-hard to find the
  optimum solution; even worse one can show, that no polynomial time
  algorithm can guarantee a solution whose cost is less than a log n
  factor more than the optimum. (based on similar complexity
  theoretical assumption $P \not= NP$)
** Approximation Ratio
- For a minimization problem (like SC), we call an algorithm A a
  $\gamma$ -approximation, if for all problem instances I, the
  algorithm A produces a result R with 
  - $c(R) \leq \gamma *c(R_{opt})$
- $\gamma$ is called the approximation guarantee of algorithm A
** Modeling SetCover as (Integer) Linear Programm
- We can formulate a setcover problem instance as an _integer_ linear
  problem.
[[file:images/Fig_51_edited.jpg]]
- Introduce for each subset $S_i$ a variable $x_i \in \{0,1\}$
  - $x_i = 0$ <=> set $S_i$ not selected
  - $x_i = 1$ <=> set $S_i$ is selected
  - $$\min \sum_{i=1}c(S_i)x_i$$
  - $$\forall u \in U: \sum_{i: u \in S_i} x_i \geq 1$$
** Natural Greedy Algorithm for set cover
- A very natural algorithm (heuristic) to get "reasonable" solutions
  for a SC instance works as follows:
  1. C<- $\emptyset$ (C denotes Elements already covered)
  2. while (C $\not=$ U)
     * for each $S_i$, let $\alpha_i = \frac{cost(S_i)}{|S_i -C|}$[fn:30]
     * let S be the set with minimal $\alpha$
     * pick S ($x_s$ = 1 ) and for each $u \in S- C$
       $y_u=\frac{\alpha}{H_n}$ [fn:31]
     * C <- $C \cup S$
  3. Output picked sets
[[file:images/Fig_52_edited.jpg]]
- numbers in parentheses correspond to circled numbers in figure above.
- Let us use LP theory to analyze the greedy algorithm.
  1. Augment greedy alg. to also compute primal integral solution (1) and
     dual solution (3).
  2. show that primal integral solution is feasible & has same
     objective function value as greedy solution (trivial)
  3. Show that dual solution is feasible
  4. Show that ratio between objective function values of primal
     integral and dual solution is $\leq H_n \approx \log n$
- ad 4: 
  - In each round, when set S is picked, primal objective function
    value increases by cost(S).
  - The dual objective function value increases by $\frac{ cost(S) }{ H_n }$
  - => at the very end, objective function value of dual solution = $$ \frac{\text{obj.fun. value of primal solution}}{H_n}$$
- ad 3:
  - We focus on one constraint for a certain subset S
  - $$ \sum_{u \in S}y_u \leq c(S)$$
  - Let us first sort the elements of the universe according to the
    time they were covered (breaking ties arbitrarily)
  - In particular for the set S we have an ordering
    $S=\{u_1,u_2,u_3,..,u_l\}$ ($u_j$ was not covered before $u_i$ for
    j>i)
  - Now consider the iteration of the greedy alg when $u_i$ was covered.
  - At that point all elements $u_i, u_{i+1},..u_l$ in S were not covered.
  - => $|S-C| \geq l-(i-1) = l + 1 - i$
  - => $\alpha_S \leq \frac{c(S)}{l+1-i}$
  - => the set S' that was picked cannot have larger $\alpha_{S'}$
    (then S would have been picked instead)
  - reminder: $y_u = \frac{\alpha}{H_n}$
  - $$ \sum_{u \in S}^{} y_u = y_u_1 +y_u_2 + ... +y_u_n =
    \sum_{i=1}^{l} y_u_i \leq \sum_{i=1}^{l} \frac{\alpha_S}{H_n} \leq
    \frac{c(s)}{(l+1-i)*H_n}$$
  - $$ \frac{1}{H_n} \sum_{i=1}^{l} \frac{1}{l+1-i} = \frac{c(S)}{H_n}
    \sum_{i=1}^{l} \frac{1}{i} \leq c(s)$$
  - $ \sum_{i=1}^{l} \frac{1}{i} = H_l$
** (Randomized) Rounding for Set Cover
- Now design algorithms which use LP machinery not only for analysis
  of the algorithm but also for the algorithm itself.
*** ( Simple ) LP-Rounding
- Algorithm
  1. Formulate the set cover problem as ILP
  2. solve LP relaxiation to optimality
  3. Pick all sets S for which $x_S > 0$ in opt. solution of LP relaxiation
- example
| x_S_1 |   0 |
| x_S_2 | 0,9 |
| x_S_3 | 0,1 |
| x_S_4 |   1 |
- => pick sets S_2, S_3, S_4 ($\hat{ =} x_S_1 = 0, x_S_2, x_S_3,
  x_S_4 = 1$)
- obviously this yields a feasible solution, quality unclear, though.
- For general set cover problem instances no good bound for the
  objective function value is known.
- But for special casses of set cover good bounds can be shown.
**** Vertice Cover
- Defintion
  - Given a graph G(V,E) (undirected) with vertex costs $c:V
    \rightarrow \mathbb{R}^+$
  - the _weighted vertex cover problem_ is to determine a subset
    $C\subseteq V$ of the nodes  such that
    - $$\forall e \in E: e \cap C \not= \emptyset$$
    - and $$ \sum_{v \in V}^{} c(v)$$ is minimal
  #+CAPTION: Red vertices form feasible solution for Vertex Cover
  [[file:images/Fig_53_edited.jpg]]
  #+CAPTION: Red vertices also form feasible solution but porbably less optimal for Vertex Cover
  [[file:images/Fig_54_edited.jpg]]
  #+CAPTION: Blue vertices form infeasible solution for Vertex Cover
  [[file:images/Fig_55_edited.jpg]]
- VC is a special case of SC (universe $\hat{=}$ set of all edges,
  subset $\hat{ =}$ edges adjacent to a node$)  [fn:32]
- ILP formulation
  - Introduce variable $x_v \in \{0,1\}$ (for picking nodes)
  - $$\min \sum_{v \in V}^{}x_v * c(v)$$
  - s.t $\forall e = \{u,v\} \in E: x_u + x_v \geq 1$
  - $x_v \in \{0,1\}$
- LP relaxiation
  - $$\min \sum_{v \in V}^{}x_v * c(v)$$
  - s.t $\forall e = \{u,v\} \in E: x_u + x_v \geq 1$
- We will first show that for the optimum solution x^* of the LP
  relaxiation we have
  - $x_v^* \in  \{ 0, \frac{1}{2}, 1\}$
  [[file:images/Fig_56_edited.jpg]]
- When rounding up all variables $x_v^* > 0$, the objective function
  value can at mode _double_ from the LP optimum
- Since LP-Optimum is a _lower bound_ to integral optimum, we get a
  solution of cost at most twice the optimum (integral) solution
- Proof
  - We will show something stronger even, namely that _all corners_ of
    the polyhedron induced by $\forall e=\{u,v\}\in E:x_u + x_v\leq 1$
    have coordinates in $\{0,0.5,1\}$
  - Lemma:
    - The corners of the polyhedron of the VC-LP relaxiation have
      coordinates in $\{0,0.5,1\}$ only.
  - Proof 
[[file:images/Fig_57_edited.jpg]]
    - a non-corner point of the polyhedron is a point which is the
      midpoint of two other points of the polyhedron.
    - If we have a point x of the polyhedron with some component
      $x_v\not\in \{0,0.5,1\}$, we will exihibit two other points of
      the polyhedron which have x as midpoint => x cannot be a corner.
    - Consider all $v \in V$ for which $x_v \not\in \{0,0.5,1\} and
      let $V^+ = \{v| \frac{1}{2} < x_v < 1\}$ $V^- = \{v | 0 < x_v < \frac{1}{2}\}$
    - For some very small $\epsilon > 0$  we define points/solutions y
      and z as follows
      - $y_v = x_v + \epsilon if v \in V^+$
      - $y_v = x_v - \epsilon if v \in V^-$
      - $y_v = x_v  \text{ otherwise }$
      - $z_v = x_v + \epsilon if v \in V^+$
      - $z_v = x_v - \epsilon if v \in V^-$
      - $z_v = x_v  \text{ otherwise }$
    - As $V^+ \cap V^- = \emptyset$ we know that x,y,z are all
      distinct. Clearly $ x = \frac{1}{2}(y+z)$.
    - Consider one edge constraint and cases
      1. $x_v + x_w > 1$ => choose $\epsilon$ small enough => constraint .....
      2. $x_v + x_w = 1$ => either $x_v = x_w = \frac{1}{2}$
         - or $v \in V^+, w \in V^-$ or the other way around
         - => in any case modification to y or z does not violate constraint.
* Exercises

** Sheet 1

*** Problem 2
- case 1: There is an s-t path with infinite capacity
  - flow from s to t is unbounded ( $$\infty$ )
- case 2: There is no s-t path with inf. capacity
  - there is an s-t-cut where all outgoing edges have finite capacity
  - FF increases total flow by at least 1 in each iteration
  - Max flow is upper bounded by a finite number
  - => Ford Fulkerson will terminate in this case

*** Problem 3
- a): Capacity and flow conservation are fulfilled, so the flow is
  valid 
- b): all edges going into t are saturated. So the flow is optimal.
  - more formal:
  - the capacity of edges in $dcut(\{s,a,b,c,d,e,f\})$ gives an upper
    bound to total flow in the network as $s \in A, t \notin A$
  - The flow matches the upper bound => optimality

*** Problem 4
- a):
  - Construct a network with s as source and t as sink
  - add a node for each student
  - add an edge from s to each student with capacity 1
  - add a node for each traineeship
  - add an edge from each traineeship to t with capacity 1
  - add edges from each student to the traineeship he/she prefers with
    capacity 1
  - If the maximum flow equals n (number of students) the preferences
    can be fulfilled.
  - a backedge from a traineeship to a student in the last residual
    network implies that the student gets the traineeship.
  - more formal:
    - set of students S $|S| = n$
    - set of traineeships T $|T| = m$
    - $ m \geq n$
[[file:images/Fig_05_edited.jpg]]
    - $cap(e) = 1 \forall e \in E$
    - if max flow = n $\rightarrow$ there is a valid assignment of
      students to traineeships
    - if $max flow < n$ $\rightarrow$ there is no such assignment
    - How to get a valid assignment:
      - $\forall e=(v,u), v \in S, u \in T$ if f(e) = 1
      - student v is assigned to traineeship u
- b):
  [[file:images/Fig_06_edited.jpg]]
  - Given directed G(V,E)
  - construct G'(V',E')
  - $\forall v \in V\backslash\{s,t\}$ introduce $v',v'' \in V'$
    $(v',v'') \in E'$
  - rename s to s'' and t to t' in G'
  - $\forall e=(v,u) \in E$ introduce $(v'',u') \in E'$
  - assign cap=1 to every Edge
  - maximum flow from s to t in G' yields the number of node-disjoint paths in G
 [[file:images/Fig_07_edited.jpg]]

** Sheet 2
*** Problem 1
**** a ) 
\begin{algorithmic}
\WHILE{$D \ge 1$}
\STATE $G_f^D \leftarrow$ residual network of $G$ restricted to edges of capacity $\ge D$ \textit{using P1/P2}
\WHILE{$\exists s-t$ path in $G_f^D$}
\STATE augment path
\STATE $G_f^D \leftarrow$ residual network with regard to $f$
\ENDWHILE
\STATE $D = \frac{D}{2}$
\ENDWHILE
\end{algorithmic}
Algorithm gives a running time $O(\log(\mbox{capa}_{\text{max}}) \cdot m \cdot (m+n))$.

Output of both procedures might differ:
[[file:images/Tutblatt_2_0001.jpg]]
**** b)
Within the inner loop edges with capacity $< D$ might occur.\\
$\Rightarrow$ it's possible to get a subgraph like this in the residual network $G_f^D$ and the running time analysis does not apply: 
[[file:images/Tutblatt_2_0002.jpg]]
[[file:images/Tutblatt_2_0003.jpg]]

Proof idea for capacity scaling running time starting the outer loop we know:
\begin{itemize}
\item all edges in $G_f^D$ have capacity of $\le 2D$ $s-t$ path
\item there are at most $m$ such paths
\item in each inner loop we augment by at least $D$
\end{itemize}
$\Rightarrow$ at most $2m$ iterations of the inner loop 

Running time of the capacity scaling algorithm is
 \[
O(\underbrace{\log(C_\text{max})}_{\text{\#outer loop iterations}} \cdot \underbrace{2m}_{\text{\#inner loop iterations}} \cdot \underbrace{(m+n)}_{\text{\#compute $G_f^D$, find $st$-path, augment}})
\]

\begin{enumerate}
\item Iteration 1 of the outer loops\\
$\Rightarrow 2D > \mbox{capa}(e) ~~~ \forall e\in E$\\
$D$ is the next smaller power of 2.
\item Iteration $i$
$\Rightarrow$ In Iteration $i-2$ $D^{i-1}$ was $2D^i$
\begin{itemize}
\item there is no more $s-t$ path in iteration $i-1$
\item every $s-t$ path in iteration $i$ have a max bottleneck capacity of $D^i = \frac{D^{i-1}}{2}$
\item there are at most $m$ $s-t$ paths $\Rightarrow$ max capacity in iteration $i$: $m\cdot 2D$
\end{itemize}
\end{enumerate}

*** Problem 2 
**** a)
Given a MinCost flow instance
\begin{align*}
G(V,E)\\
\mbox{capa}: E &\rightarrow \mathbb{N}\\
\mbox{cost}: E &\rightarrow \mathbb{Z}\\
b: V &\rightarrow \mathbb{Z}
\intertext{Construct a FF-instance:}
V' &= V \cup \{ s,t \} \\
E' &= E \cup \{ (s,v) | v \in V: b(v) > 0\} \\
&\cup \{ (v,t) | v \in V : b(v) < 0 \}\\
\mbox{capa}'(e) &= \begin{cases} \mbox{capa}(e) & \text{if} e\in E \\
b(v) & \text{if } b(v) > 0 \\
-b(u) & \text{if} b(u) < 0 \end{cases}
\end{align*}
[[file:images/Tutblatt_2_0004.jpg]]

\begin{itemize}
\item Solve max flow for $G'(V', E')$ and capa' with start node $s$ and target node $t$
\item use the computed max flow as initial flow for the cycle cancelling algorithm
\end{itemize}

**** b)
[[file:images/Tutblatt_2_0005.jpg]]

*** Problem 3
[[file:images/Tutblatt_2_0006.jpg]]

*** Problem 4
**** a) 
[[file:images/Tutblatt_2_0007.jpg]]

**** b)
- Introduce super supply $s$ and super demand $d$
- add edges from $s$ to every node $v$ with $b(v) > 0$\\
define cost$=0$, capa=$b(v)$ for the added edges
- add edges from $v$ to $t$ for every node $v$ with $b(v)<0$\\
define cost$=0$, capa$=-b(v)$ for the added edges\\

[[file:images/Tutblatt_2_0008.jpg]]
Let $b(d) = \min( \sum_{v: b(v)>0} b(v) ; \sum_{v:b(v)<0} b(v) )$\\
$b(s) = - b(d)$
** Sheet 3

*** Exercise 1

**** a)
Cost of flow $f^*$
\begin{array}{rcr}
  4*2 &= 8& \\
  0*10 &= 0& \\
  4*-5 &= -20& \\
  4*3 &= 12& \\
  7*11 &= 77& \\
  0*13 &= 0& \\
  0*6 &= 0& \\
  2*-1 &= -2& \\
  5*5 &= 25& \\
  0*5 &= 0& \\
  0*6 &= 0&\\
  0*5 &= 0& \\
  \hline
  &  100&
\end{array}
Cost of flow f
\begin{array}{rcr}
  2*2 &= 4& \\
  0*10 &= 0& \\
  0*-5 &= 0& \\
  2*3 &= 6& \\
  3*11 &= 33& \\
  0*13 &= 0& \\
  4*6 &= 24& \\
  2*-1 &= -2& \\
  7*5 &= 35& \\
  2*5 &= 10& \\
  2*6 &= 12&\\
  0*5 &= 0& \\
  \hline
  &  122&
\end{array}
- To show that f^* is optimal we need to show that $G_{ f^* }$ does
  not contain negative cycles.
- Tut 3_1 [fn:24]
- $G_{f^*}$ does _not_ contain negative cycles => $f^*$ is an optimal mcf



**** b)
- The total cost of f' must be negative as $cost(f^*)< cost(f)$.
- f' consists of one or more cylces. At least one of those cycles has
  negative costs. These cycles are also contained in $G_f$
- Results match our expectations

#+BEGIN_SRC dot :file images/graph9.jpg
  digraph g{
          rankdir=LR;
          {rank = source; a}
	  {rank = sink; g}
          {rank = same; b; c}
          {rank = same; e; f}
	  a -> b [label="2"]  
	  a -> c [label="-2"]
	  b -> d [label="2"]
	  c -> f [label="-2"]
	  d -> a [label="0"]
	  d -> e [label="4"]
	  d -> g [label="-4"]
	  f -> d [label="-2"]
	  e -> b [label="0"]
	  e -> g [label="4"]
          g -> f [label="0"]
  }
#+END_SRC

#+CAPTION: Difference Flow f'
#+RESULTS:
[[file:images/graph9.jpg]]

- Two cylces present
  - acgdba
  - degd


#+BEGIN_SRC dot :file images/graph10.jpg
  digraph g{
          rankdir=LR;
          {rank = source; a}
	  {rank = sink; g}
          {rank = same; b; c}
          {rank = same; e; f}
	  a -> b [label="2/2"]  
	  b -> a [label="2/-2"]  
	  c -> a [label="2/-5"]
	  b -> d [label="4/3"]
	  d -> b [label="2/-3"]
	  f -> c [label="2/-6"]
	  d -> a [label="3/13"]
	  d -> e [label="5/11"]
	  e -> d [label="3/-11"]
	  g -> d [label="4/-6"]
	  f -> d [label="2/5"]
	  d -> f [label="7/-5"]
	  e -> b [label="2/10"]
	  e -> g [label="5/-5"]
          g -> f [label="6/5"]
  }
#+END_SRC

#+CAPTION: Residual network $G_f$
#+RESULTS:
[[file:images/graph10.jpg]]

- cost(abdfca) = -11
- cost(dged) = 0
- MinCap(abdfca) = 2
- If we send flow around abdfca we decrease cost by 22.

*** Exercise 2 
- Tut 3_2

*** Exercise 3

#+BEGIN_SRC dot :file images/graph11.jpg
  digraph g{
	  rankdir=LR;
	  a_1 -> b_2 [label="1/2"]
	  a_1 -> c_2 [label="1/2"] 
	  b_1 -> d_2 [label="1/3"]
	  b_1 -> c_2 [label="6/10"]
	  c_1 -> b_2 [label="2/-10"]
	  c_1 -> d_2 [label="5/1"]
	  c_1 -> a_2 [label="1/-2"]
	  d_1 -> b_2 [label="2/-3"]
	  a_2 -> b_3 [label="1/2"]
	  a_2 -> c_3 [label="1/2"] 
	  b_2 -> d_3 [label="1/3"]
	  b_2 -> c_3 [label="6/10"]
	  c_2 -> b_3 [label="2/-10"]
	  c_2 -> d_3 [label="5/1"]
	  c_2 -> a_3 [label="1/-2"]
	  d_2 -> b_3 [label="2/-3"]
	  a_3 -> b_4 [label="1/2"]
	  a_3 -> c_4 [label="1/2"] 
	  b_3 -> d_4 [label="1/3"]
	  b_3 -> c_4 [label="6/10"]
	  c_3 -> b_4 [label="2/-10"]
	  c_3 -> d_4 [label="5/1"]
	  c_3 -> a_4 [label="1/-2"]
	  d_3 -> b_4 [label="2/-3"]
  }
#+END_SRC

#+RESULTS:
[[file:images/graph11.jpg]]
- Tut 3_3
- There is a circle starting in A with negative costs

*** Exercise 4

**** a)
1) What variables do we need?
   - $\forall (u,v) \in E :$ introduce a variable $f_{uv}$ that models
     f((u,v))
2) What objective do we need?
   - Decrease cost of our flow
   - $$\min \sum_{e=(u,v) \in E} cost(e)*f_{uv}$$
3) Constraints
   - Capacity constraints: -> Capacity Conservation
     - j$\forall e=(u,v) \ in E: f_{uv} \leq cap(e)$
     - $f_{uv} \geq 0$
   - vertex constraints: -> flow conservation
     - $$\forall v \in V: \sum_{\forall e=(., v)}f_{.v} + b(v) =
       \sum_{\forall e =(v,.)} f_{v.}$$
\begin{align*}
 \min &2f_{ab} + 2f_{ac} + 10f_{bc} + 3f_{bd} + 1f_{cd} \\
  \text{st}
      & f_{ab} \leq 4&  f_{ab} &\geq 0 \\
      & f_{ab} \leq 4&  f_{ab} &\geq 0 \\
      & f_{ac} \leq 2&  f_{ac} &\geq 0 \\
      & f_{bc} \leq 8&  f_{bc} &\geq 0 \\
      & f_{bd} \leq 3&  f_{bd} &\geq 0 \\
      & f_{cd} \leq 5&  f_{cd} &\geq 0 \\
\end{align*}
\begin{align*}
  4 &= f_{ab} +f_{ac} \\
  f_{ab} + 1 &= f_{bc} + f_{bd} \\
  f_{ac} +f_{bc} -3 &= f_{cd} \\
  f_{bd} +f_{ce} -2 &= 0 \\
\end{align*}

**** b)
No it is not in the standard form we learned, as it does not only
consist of less or equals  constraints and it is a minimization
problem instead of a maximization.

- Transform to standard form:
- make the objective a max.
  - $\min c^T x <=> \max -c^Tx$
#+begin_export latex
\begin{align*}
 \max &-2f_{ab} - 2f_{ac} - 10f_{bc} - 3f_{bd} - 1f_{cd} \\
\end{align*}
#+end_export
- transform constraints into '$\leq$'if constraint is $\geq 0$ then
  multiply with -1.
- if constraint '=', then split into two constraints '$\leq$' and '$\geq$'.
\begin{align*}
      & f_{ab} \leq 4&  -f_{ab} &\leq 0 \\
      & f_{ab} \leq 4&  -f_{ab} &\leq 0 \\
      & f_{ac} \leq 2&  -f_{ac} &\leq 0 \\
      & f_{bc} \leq 8&  -f_{bc} &\leq 0 \\
      & f_{bd} \leq 3&  -f_{bd} &\leq 0 \\
      & f_{cd} \leq 5&  -f_{cd} &\leq 0 \\
\end{align*}
\begin{align*}
  4 &= f_{ab} +f_{ac} \\
  f_{ab} + 1 &= f_{bc} + f_{bd} \\
  f_{ac} +f_{bc} -3 &= f_{cd} \\
  f_{bd} +f_{ce} -2 &= 0 \\
\end{align*}

**** c)
- Tut3_4c

*** Exercise 5
\begin{align*}
\max c^Tx  \\
\text{ s.t. }  Ax \leq b
\end{align*}
- feasible region is bounded and convex[fn:25] 
- => $\forall p \in P:$ p is a convex combination of corner vertices
  x_1,...x_t of P.
- Consider $x^*$ is optimal.
- $$x^* = \sum_{i=1}^{t} \lambda_i x_i$$ , $\lambda_i \geq 0$,
  $$\sum_{i=1}^{t} \lambda_i = 1$$
- $$c^T x^* = c^T \sum_{i=1}^{t} \lambda_i x_i$$
- <=> $$0 = c^T \sum \lambda_i x_i - c^T x^*$$
- $$0 = c^T(\sum \lambda_i x_i - x^*)$$
  - $$\sum \lambda_i = 1$$
- <=> $$ 0 = c^T (\sum \lambda_i (x_i - x^*))$$
- <=> $$0= \sum \lambda_i (c^Tx_i - c^Tx^*)$$ (Satz vom Nullprodukt)
- $\exists \lambda_i \not= 0$ => $\exists x_i: c^Tx_i = c^Tx^*$
** Sheet 4
*** Exersice 1
**** Part 1
Assume LP P is in standard form 
\begin{align*}
\max c^Tx \\
Ax \leq b
\end{align*}
assume $b \geq 0$
Find an inital corner of the feasible region.
Construct auxiliary LP $P_A$
- introduce variables $\lambda_i$ for every constraint in the original
  problem P
\begin{align*}
\min \sum \lambda \\
\text{ s.t. } Ax+ \lambda \leq b\\
\lambda \geq 0
\end{align*}
- A corner of the auxiliary problem is at x = 0 and $\lambda = b$
- Figure out a solution (corner) of P:
- solve auxiliary Problem using Simplex and the given starting
  position.
- obj. Function value of $P_A$ is bounded by 0.
- if $P_A$ has an optimal solution = 0 => it implies a valid corner
  point of the original problem P
- take the constraint that are fulfilled with '=' as a basis for P.
**** Part 2
- Find an initial v-shape
- Let LP P be in standard from:
\begin{align*}
\max c^Tx \\
Ax \leq b
\end{align*}
- construct an aux. problem $P_A$
\begin{align*}
\max &( c^Tx ) \\
\text{s.t. } &Ax \leq 0 \\
&c^T x \leq 1
\end{align*}
- This moves all the constraints so that they cross the origin (0,0...0)
#+CAPTION: Initial constraints in blue, modified and new constraints in red.
[[file:images/Tut_4_1.jpg]]
- an initial v-shape of $P_A$ is defined by $c^Tx \leq 1$ and $d-1$ linear
  independent constraints of P.
- if the dual simplex terminates with a v-shape in the origin => the
  d-constraint from a valid v-shape in P.
*** Exercise 2
\begin{align*}
\max c^Tx \\
\text{s.t. } Ax \leq b
\end{align*}
- -> dualize
\begin{align*}
\min b^Ty \\
\text{s.t} A^Ty = c \\
y \geq 0
\end{align*}
- bring into standard from
\begin{align*}
\max -b^Ty \\
\text{s.t. } A^Ty \leq c  :x'\\  
-A^T y \leq -c :x''\\  
- y \leq 0 :x'''
\end{align*}
- dualize again
- 
\begin{align*}
\min c^Tx' -c^Tx''-0x''' \\
\text{s.t. } Ax' - Ax'' - 1 x''' = -b \\
x',x'',x''' \geq 0
\end{align*}
- drop $x'''$

\begin{align*}
\min c^Tx' -x^Tx'' \\
\text{s.t. }  Ax' - Ax'' \geq -b \\
x',x'' \geq 0
\end{align*}

\begin{align*}
\min c^T(x'-x'') \\
\text{s.t. } A(x'-x'') \geq -b \\
x',x'' \geq 0
\end{align*}

- Substitute $x = x'' - x'$
\begin{align*}
\min c^T(-x)
\text{s.t. } A*(-x) \geq -b
\end{align*}
- multiply with -1
\begin{align*}
\max c^T x \\
Ax \leq b
\end{align*}
*** Exercise 3
\begin{align*}
\max x_1 +x_2 +x_3 \\
\text{s.t. } -\frac{3}{2}x_1 + 0x_2 + x_3 \leq 1.5 \\
\frac{3}{2}x_2 +x_3 \leq 6 \\
x_1 \leq 5 \\
-x_2 \leq 0 \\
-x_3 \leq 0 \\
\end{align*}
- Basis of x' contains the constraints III, II, V
- Releasing some of the constraints is related to increasing s (slack variable):
- The following corner point fulfills the constraints II,III, I
[[file:images/Tut_4_2.jpg]]
*** Exercise 4 
- Solution can be found on the updated exercise [[http://www.fmi.uni-stuttgart.de/fileadmin/user_upload/fmi/alg/lehre/ws16/dopt/exercises/Exercise04.pdf][sheet]].
 
* Footnotes

[fn:32] common exam question: which is a special case of what?

[fn:31] $H_n$ n-th harmonic number $\approx$ log n

[fn:30] $|S_i-C|$: # elements newly covered by $S_i$. $\alpha_i$: cost
per new element.

[fn:29] red constraint is added by the ellipsoid method to find a
lower feasible point

[fn:28] Not part of the exam (probably in Conputational Geometry)

[fn:27] Unlikely that this proof will be asked in the exam.

[fn:26] $\leq n$ non-zero components (of the constraints defining V-Shape)

[fn:25] Definition of convex: $\forall p \not= q \in P$ where P is
convex. $p + \alpha* \overarrow{pq}$  with $0 \leq \alpha  \leq 1$ is
also in P. Every Point on the line from p to q is also in P.
 
[fn:24] In the exam its sufficient to draw the residual network and
state that there are no negative cycles.

[fn:23] In the exam almost for sure: Show that the dual of the dual is
the primal. He wants us to use his standard form definition not
transformation rules we can find in the internet.

[fn:22] In theory often not interesting (we ignore degenerate
cases). In practice this is very important.

[fn:21] Exercise: Try to make cyclic example in $\mathbb{R}^2$. Where
you sit in a corner of mutliple constraints forever.

[fn:20] drawing a correct coordinate system is important (points could
get deducted). Also you need to be able to read of the solution from
your drawing.

[fn:19] In the exam drawing a linear program (feasible region) is
always asked

[fn:18] Possible exam question: transform this linear program into
standard form

[fn:17] He inconsistently uses n and d as dimension

[fn:16] Obvious, since it is like computing first feasible flow for
cycle cancelling

[fn:15] Approach will never violate the capacity constraints. But it
will not always use all the supplies and all the demands.

[fn:14] There could be a modelling exercise in the exam

[fn:13] There is a connection to the set A in the proof of Ford
Fulkerson. (A plays the roll of S)


[fn:12] Given a graph G(V,E), does there exist a simple cycle
containing all nodes.

[fn:11] if and only if

[fn:10] In Exam probably at least one question where MaxFlow or
MinCostFlow is computed step by step by hand

[fn:9] used to its full capacity

[fn:8] Sequence of states/augmentations

[fn:7] Connections are made from higher level to lower level. So
shortest path is not decreased.

[fn:6] by removing edges from a network, the distance cannot be decreased.

[fn:5] correctness proof of FF is common Exam question. 

[fn:4] In Exam sometimes: Why is it optimal what you have computed?
The answer is not that in the residual network there is no path from s
to t. 

[fn:3] |E| = m & |V| = n

[fn:2] typical exam question: Why does Ford Fulkerson terminate?

[fn:1] e_R = Reverse edge 
